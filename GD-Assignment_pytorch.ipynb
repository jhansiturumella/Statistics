{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pytorch torchvision cudatoolkit=9.0 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorboardX in c:\\users\\umaraj\\appdata\\roaming\\python\\python37\\site-packages (1.8)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.2.0 in c:\\users\\umaraj\\appdata\\roaming\\python\\python37\\site-packages (from tensorboardX) (3.9.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in c:\\users\\umaraj\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorboardX) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\umaraj\\appdata\\roaming\\python\\python37\\site-packages (from tensorboardX) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\umaraj\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from protobuf>=3.2.0->tensorboardX) (40.8.0)\n"
     ]
    }
   ],
   "source": [
    "cmd = \"pip install --upgrade tensorboardX --user \"\n",
    "pw = \"data\"\n",
    "!echo {pw}|  {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accessing predefined path\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "log_path = './runs/gd/'\n",
    "\n",
    "if log_path:\n",
    "    print(\"accessing predefined path\")\n",
    "    writer = SummaryWriter(log_dir=log_path)\n",
    "else :\n",
    "    print(\"using new path set\")\n",
    "    writer = SummaryWriter(log_dir='./runs/gd/')\n",
    "#In addition to SummaryWriter, there are also other writers, please check the manual\n",
    "# https://tensorboardx.readthedocs.io/en/latest/tutorial.html\n",
    "\n",
    "# !tensorboard --logdir log_path --host localhost --port 8088\n",
    "# you have to execute tensorboard command from another shell, otherwise you cannot proceed with running the notebook\n",
    "# read this : https://medium.com/@anthony_sarkis/tensorboard-quick-start-in-5-minutes-e3ec69f673af\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 326349440.0\n",
      "1 201279936.0\n",
      "2 61422464.0\n",
      "3 29971142.0\n",
      "4 22837732.0\n",
      "5 18258276.0\n",
      "6 15047410.0\n",
      "7 12691768.0\n",
      "8 10897928.0\n",
      "9 9487210.0\n",
      "10 8357455.5\n",
      "11 7437136.5\n",
      "12 6674760.5\n",
      "13 6033801.0\n",
      "14 5487987.0\n",
      "15 5019714.0\n",
      "16 4613828.5\n",
      "17 4259200.5\n",
      "18 3947277.75\n",
      "19 3668568.0\n",
      "20 3420318.75\n",
      "21 3198021.75\n",
      "22 2998589.5\n",
      "23 2818985.75\n",
      "24 2656714.75\n",
      "25 2509054.0\n",
      "26 2374250.25\n",
      "27 2250205.25\n",
      "28 2136497.0\n",
      "29 2031936.25\n",
      "30 1935466.125\n",
      "31 1846240.875\n",
      "32 1763509.5\n",
      "33 1686316.5\n",
      "34 1614480.5\n",
      "35 1547380.125\n",
      "36 1484706.125\n",
      "37 1426029.375\n",
      "38 1370799.375\n",
      "39 1318926.125\n",
      "40 1270079.75\n",
      "41 1223930.25\n",
      "42 1180458.0\n",
      "43 1139130.125\n",
      "44 1100172.5\n",
      "45 1063332.75\n",
      "46 1028471.375\n",
      "47 995368.875\n",
      "48 963789.375\n",
      "49 933811.25\n",
      "50 905302.625\n",
      "51 878049.8125\n",
      "52 852130.5625\n",
      "53 827417.25\n",
      "54 803708.3125\n",
      "55 781095.125\n",
      "56 759506.875\n",
      "57 738827.75\n",
      "58 719065.25\n",
      "59 700122.75\n",
      "60 681965.1875\n",
      "61 664543.0625\n",
      "62 647816.4375\n",
      "63 631744.1875\n",
      "64 616294.0625\n",
      "65 601450.5\n",
      "66 587166.4375\n",
      "67 573405.3125\n",
      "68 560133.125\n",
      "69 547290.875\n",
      "70 534912.625\n",
      "71 522985.40625\n",
      "72 511303.8125\n",
      "73 500085.75\n",
      "74 489278.3125\n",
      "75 478858.375\n",
      "76 468802.625\n",
      "77 459069.375\n",
      "78 449661.15625\n",
      "79 440554.59375\n",
      "80 431741.25\n",
      "81 423194.90625\n",
      "82 414879.53125\n",
      "83 406831.3125\n",
      "84 399027.15625\n",
      "85 391463.96875\n",
      "86 384124.625\n",
      "87 376999.5625\n",
      "88 370071.90625\n",
      "89 363353.75\n",
      "90 356819.28125\n",
      "91 350463.0\n",
      "92 344284.375\n",
      "93 338276.625\n",
      "94 332430.625\n",
      "95 326739.3125\n",
      "96 321197.53125\n",
      "97 315798.28125\n",
      "98 310536.5\n",
      "99 305408.125\n",
      "100 300405.90625\n",
      "101 295528.625\n",
      "102 290775.0\n",
      "103 286136.5625\n",
      "104 281558.125\n",
      "105 277105.03125\n",
      "106 272764.96875\n",
      "107 268537.0\n",
      "108 264415.25\n",
      "109 260387.3125\n",
      "110 256454.9375\n",
      "111 252617.84375\n",
      "112 248861.65625\n",
      "113 245192.828125\n",
      "114 241606.734375\n",
      "115 238100.453125\n",
      "116 234672.890625\n",
      "117 231319.453125\n",
      "118 228039.6875\n",
      "119 224832.03125\n",
      "120 221692.96875\n",
      "121 218617.96875\n",
      "122 215607.0\n",
      "123 212660.578125\n",
      "124 209778.375\n",
      "125 206953.21875\n",
      "126 204185.375\n",
      "127 201472.703125\n",
      "128 198811.09375\n",
      "129 196200.953125\n",
      "130 193643.8125\n",
      "131 191135.25\n",
      "132 188675.359375\n",
      "133 186266.875\n",
      "134 183905.109375\n",
      "135 181587.34375\n",
      "136 179313.171875\n",
      "137 177081.4375\n",
      "138 174893.078125\n",
      "139 172745.796875\n",
      "140 170636.609375\n",
      "141 168567.25\n",
      "142 166534.75\n",
      "143 164538.28125\n",
      "144 162581.546875\n",
      "145 160658.171875\n",
      "146 158768.640625\n",
      "147 156886.296875\n",
      "148 155042.984375\n",
      "149 153237.296875\n",
      "150 151463.625\n",
      "151 149724.1875\n",
      "152 148015.40625\n",
      "153 146337.84375\n",
      "154 144686.5625\n",
      "155 143063.828125\n",
      "156 141470.34375\n",
      "157 139904.5625\n",
      "158 138363.484375\n",
      "159 136849.0\n",
      "160 135359.65625\n",
      "161 133895.25\n",
      "162 132455.15625\n",
      "163 131038.953125\n",
      "164 129644.671875\n",
      "165 128273.5625\n",
      "166 126923.640625\n",
      "167 125596.8828125\n",
      "168 124289.75\n",
      "169 123002.6171875\n",
      "170 121736.203125\n",
      "171 120488.96875\n",
      "172 119261.34375\n",
      "173 118052.65625\n",
      "174 116861.40625\n",
      "175 115688.140625\n",
      "176 114532.703125\n",
      "177 113395.984375\n",
      "178 112276.21875\n",
      "179 111172.25\n",
      "180 110085.5390625\n",
      "181 109014.21875\n",
      "182 107960.09375\n",
      "183 106921.921875\n",
      "184 105898.3671875\n",
      "185 104888.9140625\n",
      "186 103893.90625\n",
      "187 102913.4765625\n",
      "188 101946.6953125\n",
      "189 100993.96875\n",
      "190 100055.4140625\n",
      "191 99128.8671875\n",
      "192 98215.2578125\n",
      "193 97314.5234375\n",
      "194 96426.1953125\n",
      "195 95550.625\n",
      "196 94687.859375\n",
      "197 93835.3828125\n",
      "198 92994.3671875\n",
      "199 92165.0859375\n",
      "200 91347.15625\n",
      "201 90540.09375\n",
      "202 89744.078125\n",
      "203 88957.1640625\n",
      "204 88181.5625\n",
      "205 87414.765625\n",
      "206 86658.125\n",
      "207 85911.5234375\n",
      "208 85174.8125\n",
      "209 84448.359375\n",
      "210 83730.59375\n",
      "211 83022.4453125\n",
      "212 82323.3125\n",
      "213 81632.546875\n",
      "214 80951.296875\n",
      "215 80278.3671875\n",
      "216 79613.5\n",
      "217 78957.2265625\n",
      "218 78308.7109375\n",
      "219 77668.7890625\n",
      "220 77036.828125\n",
      "221 76412.3828125\n",
      "222 75795.28125\n",
      "223 75185.890625\n",
      "224 74584.9296875\n",
      "225 73990.0625\n",
      "226 73402.3359375\n",
      "227 72821.8203125\n",
      "228 72247.90625\n",
      "229 71681.078125\n",
      "230 71121.40625\n",
      "231 70568.2421875\n",
      "232 70021.859375\n",
      "233 69481.234375\n",
      "234 68946.796875\n",
      "235 68418.5234375\n",
      "236 67896.546875\n",
      "237 67380.6796875\n",
      "238 66870.5\n",
      "239 66365.96875\n",
      "240 65867.7421875\n",
      "241 65375.16015625\n",
      "242 64887.6484375\n",
      "243 64406.14453125\n",
      "244 63929.48828125\n",
      "245 63458.390625\n",
      "246 62992.734375\n",
      "247 62532.37109375\n",
      "248 62077.00390625\n",
      "249 61626.734375\n",
      "250 61181.1875\n",
      "251 60740.50390625\n",
      "252 60304.9921875\n",
      "253 59873.7109375\n",
      "254 59447.2109375\n",
      "255 59025.12109375\n",
      "256 58607.62109375\n",
      "257 58194.828125\n",
      "258 57786.640625\n",
      "259 57382.94921875\n",
      "260 56983.4140625\n",
      "261 56587.9296875\n",
      "262 56196.44140625\n",
      "263 55809.17578125\n",
      "264 55425.80859375\n",
      "265 55046.65625\n",
      "266 54671.57421875\n",
      "267 54300.44140625\n",
      "268 53932.90234375\n",
      "269 53568.9921875\n",
      "270 53208.68359375\n",
      "271 52852.359375\n",
      "272 52500.20703125\n",
      "273 52151.33984375\n",
      "274 51805.83984375\n",
      "275 51464.109375\n",
      "276 51125.3046875\n",
      "277 50790.0703125\n",
      "278 50458.3828125\n",
      "279 50128.70703125\n",
      "280 49798.08984375\n",
      "281 49472.0\n",
      "282 49150.1171875\n",
      "283 48831.7265625\n",
      "284 48516.9921875\n",
      "285 48205.75\n",
      "286 47897.62109375\n",
      "287 47592.6484375\n",
      "288 47290.50390625\n",
      "289 46991.44921875\n",
      "290 46695.578125\n",
      "291 46402.91796875\n",
      "292 46113.1953125\n",
      "293 45825.83203125\n",
      "294 45541.3125\n",
      "295 45259.76953125\n",
      "296 44980.7421875\n",
      "297 44704.203125\n",
      "298 44430.328125\n",
      "299 44159.19140625\n",
      "300 43890.4296875\n",
      "301 43624.3125\n",
      "302 43361.0625\n",
      "303 43100.09375\n",
      "304 42841.4453125\n",
      "305 42585.27734375\n",
      "306 42331.45703125\n",
      "307 42079.9765625\n",
      "308 41831.02734375\n",
      "309 41584.3203125\n",
      "310 41339.6875\n",
      "311 41097.3984375\n",
      "312 40857.46875\n",
      "313 40619.8515625\n",
      "314 40384.46875\n",
      "315 40150.87109375\n",
      "316 39919.2265625\n",
      "317 39689.80078125\n",
      "318 39462.65234375\n",
      "319 39237.7265625\n",
      "320 39014.44921875\n",
      "321 38793.109375\n",
      "322 38573.8125\n",
      "323 38356.6640625\n",
      "324 38141.10546875\n",
      "325 37927.74609375\n",
      "326 37716.29296875\n",
      "327 37506.48828125\n",
      "328 37298.53125\n",
      "329 37092.31640625\n",
      "330 36887.78515625\n",
      "331 36685.140625\n",
      "332 36484.859375\n",
      "333 36285.69921875\n",
      "334 36088.5703125\n",
      "335 35893.0234375\n",
      "336 35699.0546875\n",
      "337 35506.47265625\n",
      "338 35315.62109375\n",
      "339 35126.6640625\n",
      "340 34939.1484375\n",
      "341 34753.15625\n",
      "342 34568.68359375\n",
      "343 34385.9375\n",
      "344 34204.3125\n",
      "345 34024.40234375\n",
      "346 33846.03125\n",
      "347 33669.203125\n",
      "348 33493.7578125\n",
      "349 33319.6875\n",
      "350 33147.07421875\n",
      "351 32975.9375\n",
      "352 32806.171875\n",
      "353 32637.806640625\n",
      "354 32470.955078125\n",
      "355 32305.443359375\n",
      "356 32141.201171875\n",
      "357 31978.244140625\n",
      "358 31816.66796875\n",
      "359 31656.435546875\n",
      "360 31497.375\n",
      "361 31339.607421875\n",
      "362 31183.228515625\n",
      "363 31028.146484375\n",
      "364 30874.404296875\n",
      "365 30721.859375\n",
      "366 30570.1796875\n",
      "367 30419.708984375\n",
      "368 30270.533203125\n",
      "369 30122.490234375\n",
      "370 29975.583984375\n",
      "371 29830.056640625\n",
      "372 29685.640625\n",
      "373 29542.35546875\n",
      "374 29400.15234375\n",
      "375 29258.90234375\n",
      "376 29118.75\n",
      "377 28979.80078125\n",
      "378 28841.859375\n",
      "379 28704.978515625\n",
      "380 28569.158203125\n",
      "381 28434.296875\n",
      "382 28300.50390625\n",
      "383 28167.673828125\n",
      "384 28035.826171875\n",
      "385 27905.013671875\n",
      "386 27775.20703125\n",
      "387 27646.505859375\n",
      "388 27518.646484375\n",
      "389 27391.736328125\n",
      "390 27265.74609375\n",
      "391 27140.84765625\n",
      "392 27016.892578125\n",
      "393 26893.689453125\n",
      "394 26771.349609375\n",
      "395 26649.859375\n",
      "396 26529.32421875\n",
      "397 26409.8046875\n",
      "398 26291.185546875\n",
      "399 26173.251953125\n",
      "400 26056.1953125\n",
      "401 25940.140625\n",
      "402 25824.876953125\n",
      "403 25710.388671875\n",
      "404 25596.7734375\n",
      "405 25483.98046875\n",
      "406 25371.970703125\n",
      "407 25260.7578125\n",
      "408 25150.3515625\n",
      "409 25040.77734375\n",
      "410 24931.994140625\n",
      "411 24823.94140625\n",
      "412 24716.658203125\n",
      "413 24610.1015625\n",
      "414 24504.326171875\n",
      "415 24399.2578125\n",
      "416 24294.939453125\n",
      "417 24191.46484375\n",
      "418 24088.619140625\n",
      "419 23986.46875\n",
      "420 23885.0546875\n",
      "421 23784.3359375\n",
      "422 23684.453125\n",
      "423 23585.2265625\n",
      "424 23486.59765625\n",
      "425 23388.72265625\n",
      "426 23291.578125\n",
      "427 23195.00390625\n",
      "428 23099.083984375\n",
      "429 23003.8828125\n",
      "430 22909.298828125\n",
      "431 22815.373046875\n",
      "432 22722.208984375\n",
      "433 22629.6328125\n",
      "434 22537.75\n",
      "435 22446.42578125\n",
      "436 22355.646484375\n",
      "437 22265.4375\n",
      "438 22175.87890625\n",
      "439 22086.9296875\n",
      "440 21998.513671875\n",
      "441 21910.759765625\n",
      "442 21823.61328125\n",
      "443 21736.9296875\n",
      "444 21650.83203125\n",
      "445 21565.353515625\n",
      "446 21480.498046875\n",
      "447 21396.17578125\n",
      "448 21312.2890625\n",
      "449 21229.169921875\n",
      "450 21146.560546875\n",
      "451 21064.48828125\n",
      "452 20982.8984375\n",
      "453 20901.8203125\n",
      "454 20821.2421875\n",
      "455 20741.201171875\n",
      "456 20661.671875\n",
      "457 20582.677734375\n",
      "458 20504.220703125\n",
      "459 20426.30078125\n",
      "460 20348.798828125\n",
      "461 20271.798828125\n",
      "462 20195.296875\n",
      "463 20119.265625\n",
      "464 20043.763671875\n",
      "465 19968.689453125\n",
      "466 19894.2109375\n",
      "467 19820.0859375\n",
      "468 19746.46875\n",
      "469 19673.275390625\n",
      "470 19600.65234375\n",
      "471 19528.470703125\n",
      "472 19456.65234375\n",
      "473 19385.30859375\n",
      "474 19314.515625\n",
      "475 19244.1796875\n",
      "476 19174.296875\n",
      "477 19104.78125\n",
      "478 19035.77734375\n",
      "479 18967.1953125\n",
      "480 18898.986328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481 18831.119140625\n",
      "482 18763.7109375\n",
      "483 18696.720703125\n",
      "484 18630.06640625\n",
      "485 18563.87890625\n",
      "486 18498.150390625\n",
      "487 18432.7421875\n",
      "488 18366.1015625\n",
      "489 18300.375\n",
      "490 18235.28515625\n",
      "491 18170.833984375\n",
      "492 18106.95703125\n",
      "493 18043.462890625\n",
      "494 17980.337890625\n",
      "495 17917.619140625\n",
      "496 17855.26953125\n",
      "497 17793.345703125\n",
      "498 17731.796875\n",
      "499 17670.65234375\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/two_layer_net_autograd.py\n",
    "import torch\n",
    "\n",
    "#device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "device = torch.device('cpu') # Uncomment this to run on CPU\n",
    "\n",
    "# N is batch size;\n",
    "# D_in is input dimension;\n",
    "# H is hidden dimension; \n",
    "#D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 100\n",
    " \n",
    "# input data :batch of 64 times 1000 features\n",
    "# output data : 100 x 10 continues values (real scalars)\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device, dtype=torch.float)# generate normally distributed data of dim NxD_in, store it on device, requires_grad = False\n",
    "y = torch.randn(N, D_out, device=device, dtype=torch.float)# generate normally distributed data of dim NxD_out, store it on device, requires_grad = False\n",
    "\n",
    "#y_obs = y + 0.2*torch.randn(N,1)\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "# here the gradient will be computed for the variables that are related to the model learning something new,\n",
    "# i.e. the network weights in this case\n",
    "\n",
    "\n",
    "# WEIGHTS\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=torch.float, requires_grad=True)#generate normally distributed data of dim D_in x H, store it on device, requires_grad = True \n",
    "w2 = torch.randn(D_out, H, device=device, dtype=torch.float, requires_grad=True)#generate normally distributed data of dim H x D_out, store it on device, requires_grad = True \n",
    "\n",
    "# initialize loss value to a high number \n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    print(t, loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "    \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "# initialize arrays errors, w1_array and w2_array to empty lists\n",
    "#errors = # write here\n",
    "#w1_array =  # write here\n",
    "#w2_array =  # write here\n",
    " # set the network learning rate parameter 'learning_rate' to some small number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "for iteration in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.\n",
    "    # predict the values by multiplying x with weight matrix w1, then apply RELU activation and multiply the result by weight matrix w2\n",
    "    y_pred = F.relu(x.mm(w1).clamp(min=0).mm(w2))# your code here ; the final prediction is given by matrix multiplying the data \n",
    "    #with the two set of weights, making the intermediate values non-negative (RELU activation function)\n",
    "\n",
    "    # calculate the mean squared error (MSE)\n",
    "    error =  # your code here\n",
    "\n",
    "    \n",
    "    writer.add_scalar(tag=\"Last run\",scalar_value= error, global_step = iteration)\n",
    "    writer.add_histogram(\"error distribution\",error)\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "   \n",
    "    #error.WHAT_FUNCTION_here ?\n",
    "\n",
    "    # Update weights using gradient descent. For this step we just want to mutate\n",
    "    # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "    # to prevent PyTorch from building a computational graph for the updates\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # use w1.grad to update w2 according to the gradient descent formula\n",
    "        # use w2.grad to update w2 according to the gradient descent formula\n",
    "        # also use the learning_rate you set before!\n",
    "        w1 -= learning_rate * w1.grad# your code here\n",
    "        w2 -= learning_rate * w2.grad# your code here\n",
    "        \n",
    "    if iteration % 50 == 0:\n",
    "        print(\"Iteration: %d - Error: %.4f\" % (iteration, error))\n",
    "        w1_array.append(w1.cpu().detach().numpy())\n",
    "        w2_array.append(w2.cpu().detach().numpy())\n",
    "        errors.append(error.cpu().detach().numpy())\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()\n",
    "    if loss_value < 1e-6:\n",
    "        print(\"Stopping gradient descent, algorithm converged, MSE loss is smaller than 1E-6\")\n",
    "        break\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In your own words, what is the relation between backpropagation used in neural networks and Automatic Differentiation ?__\n",
    "\n",
    "Backpropagation is a supervisied learning algorithm used to train neural networks to find the least value of the error function. It uses the gradient descent method  and the weights with the least error function are considered to be the answer to the learning problem. It calculates the weights update in order to improve the network until it can perform itsâ€™ task. Backpropagation requires the derivative of the activation functions to be known at design time.\n",
    "Automatic differentiation is a method that can automatically and analytically provide the derivatives to the training algorithm.\n",
    "It provides the derivatives for the backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
